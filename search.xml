<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>线性回归的从零开始实现</title>
      <link href="/2022/03/02/deepLearning/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/"/>
      <url>/2022/03/02/deepLearning/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>以下都是根据大牛李沐老师的动手学深度学习课程里面学来的 <script type="math/tex">\sigma</script></p><h1 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h1><p>线性回归的从零开始实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure></p><h1 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h1><p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵 $ X \in R^{1000\times2} $ 。</p><p>我们使用线性模型参数 $ w = [2,-3.4]^T 、b=4.2$  和噪声项 ϵ 生成数据集及其标签：</p><script type="math/tex; mode=display">y=Xw+b+ϵ</script><p>你可以将 ϵ 视为模型预测和标签时的潜在观测误差。 在这里我们认为标准假设成立，即 ϵ 服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。 下面的代码生成合成数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>注意，features中的每一行都包含一个二维数据样本， labels中的每一行都包含一维标签值（一个标量）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">features[:<span class="number">10</span>], labels[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>(tensor([[-0.9976,  0.1518],         [-0.1213, -0.7667],         [ 0.7113, -0.2407],         [ 0.9693,  0.3422],         [-0.1454,  0.7636],         [ 1.2779,  2.1234],         [ 0.7914, -0.2531],         [ 1.5708,  0.0926],         [-0.0097, -0.0881],         [ 0.9327,  0.9346]]), tensor([[ 1.6867],         [ 6.5508],         [ 6.4413],         [ 4.9765],         [ 1.3117],         [-0.4774],         [ 6.6367],         [ 7.0211],         [ 4.4742],         [ 2.8960]]))</code></pre><p>通过生成第二个特征features[:, 1]和labels的散点图， 可以直观观察到两者之间的线性关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(features[:, (<span class="number">1</span>)].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><img src="/images/deepLearning/1.png" alt="" style="zoom: 150%;" /><br>​    </p><h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><p>回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。</p><p>在下面的代码中，我们定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量。 每个小批量包含一组特征和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br></pre></td></tr></table></figure><p>通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。 每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。 GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。</p><p>我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。 每个批量的特征维度显示批量大小和输入特征数。 同样的，批量的标签形状与batch_size相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[-1.1005, -1.1098],        [-1.4508,  0.2098],        [-0.3707,  0.1753],        [-1.3526,  0.1829],        [-0.7242,  0.3607],        [ 2.6414, -0.5502],        [-0.0335,  0.2794],        [ 0.0572,  2.7126],        [ 2.8010, -0.6305],        [-0.1200,  1.5621]])  tensor([[ 5.7428],        [ 0.6024],        [ 2.8546],        [ 0.8543],        [ 1.5218],        [11.3366],        [ 3.1882],        [-4.9111],        [11.9449],        [-1.3620]])</code></pre><p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。</p><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><p>在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w, b</span><br></pre></td></tr></table></figure><pre><code>(tensor([[-0.0105],         [-0.0104]], requires_grad=True), tensor([0.], requires_grad=True))</code></pre><p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。 我们使用 2.5节中引入的自动微分来计算梯度。</p><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。 回想一下，要计算线性模型的输出， 我们只需计算输入特征 X 和模型权重 w 的矩阵-向量乘法后加上偏置 b 。 注意，上面的 Xw 是一个向量，而 b 是一个标量。 回想一下 2.1.3节中描述的广播机制： 当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span>(<span class="params">X, w, b</span>):</span>  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span>(<span class="params">y_hat, y</span>):</span>  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h2><p>正如我们在 3.1节中讨论的，线性回归有解析解。 尽管线性回归有解析解，但本书中的其他模型却没有。 这里我们介绍小批量随机梯度下降。</p><p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。 该函数接受模型参数集合、学习速率和批量大小作为输入。每 一步更新的大小由学习速率lr决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（batch_size） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">params, lr, batch_size</span>):</span>  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。 理解这段代码至关重要，因为从事深度学习后， 你会一遍又一遍地看到几乎相同的训练过程。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法sgd来更新模型参数。</p><p>概括一下，我们将执行以下循环：</p><p>初始化参数</p><p>重复以下训练，直到完成</p><p>计算梯度 $ g←∂<em>{(w,b)}\frac{1}{|\beta|}\sum</em>{i \in \beta}l(x(i),y(i),w,b) $</p><p>更新参数$ (w,b)←(w,b)−ηg $</p><p>在每个迭代周期（epoch）中，我们使用data_iter函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。 这里的迭代周期个数num_epochs和学习率lr都是超参数，分别设为3和0.03。 设置超参数很棘手，需要通过反复试验进行调整。 我们现在忽略这些细节，以后会在 11节中详细介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.028756epoch 2, loss 0.000097epoch 3, loss 0.000051</code></pre><p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。 事实上，真实参数和通过训练学到的参数确实非常接近。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>w的估计误差: tensor([-2.0027e-05, -5.3644e-05], grad_fn=&lt;SubBackward0&gt;)b的估计误差: tensor([0.0010], grad_fn=&lt;RsubBackward1&gt;)</code></pre><p>注意，我们不应该想当然地认为我们能够完美地求解参数。 在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。 幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。 其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。</p><p>这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用的一些命令（持续跟新）</title>
      <link href="/2022/02/27/%E7%8E%AF%E5%A2%83%E6%93%8D%E4%BD%9C/environment/"/>
      <url>/2022/02/27/%E7%8E%AF%E5%A2%83%E6%93%8D%E4%BD%9C/environment/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux修改pip镜像源"><a href="#Linux修改pip镜像源" class="headerlink" title="Linux修改pip镜像源"></a>Linux修改pip镜像源</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir .pip</span><br><span class="line">vim pip.conf</span><br></pre></td></tr></table></figure><p>输入以下内容，然后保存退出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure><p>查看pip下载时的镜像源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip config list</span><br></pre></td></tr></table></figure><p>pip卸载包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pandas</span><br></pre></td></tr></table></figure><p>pip更新包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U(--upgrade) packageName</span><br></pre></td></tr></table></figure><h2 id="conda有关的操作"><a href="#conda有关的操作" class="headerlink" title="conda有关的操作"></a>conda有关的操作</h2><p>conda重命名一个环境：</p><p>conda没有重命名指令，需要通过clone完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步，创建一个新环境，并复制你需要的环境</span></span><br><span class="line">conda create -n tf_new --<span class="built_in">clone</span> tf_old</span><br><span class="line"><span class="comment"># 第二步，删除旧环境</span></span><br><span class="line">conda remove -n tf_old--all</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 命令行 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
            <tag> conda </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习一些基本理念</title>
      <link href="/2022/02/24/deepLearning/AI%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2022/02/24/deepLearning/AI%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>y为标量，x为向量，常用的求导样例<img src="/images/postImages/3.png" alt=""></p><p>y为向量，x为向量，A为矩阵，常用的求导样例</p><p><img src="/images/postImages/4.png" alt=""></p><p>分子布局，向量和矩阵的求导关系<br><img src="/images/postImages/1.png" alt="" style="zoom: 80%;" /></p><p>正向传递和反向传递<br>需要保存正向传递的结果，反向传递求导时需要读取正向传递保存的结果<img src="/images/postImages/2.png" alt=""></p><p>梯度是使得函数值增加最快的方向，那么负梯度就是使得函数值下降最快的方向</p><p><img src="/images/postImages/5.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础语法（持续更新）</title>
      <link href="/2022/02/16/python/python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
      <url>/2022/02/16/python/python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="初始化一个二维数组"><a href="#初始化一个二维数组" class="headerlink" title="初始化一个二维数组"></a>初始化一个二维数组</h2><p>初始化一个2*3，内容为#的数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initial_array = [[<span class="string">&#x27;#&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br></pre></td></tr></table></figure><h2 id="join-函数-连接字符串"><a href="#join-函数-连接字符串" class="headerlink" title="join() 函数 连接字符串"></a>join() 函数 连接字符串</h2><p>连接字符串数组。将字符串、元组、列表中的元素以指定的字符(分隔符)连接生成一个新的字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seq1 = [<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;good&#x27;</span>,<span class="string">&#x27;boy&#x27;</span>,<span class="string">&#x27;doiido&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;:&#x27;</span>.join(seq1))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">hello:good:boy:doiido</span><br></pre></td></tr></table></figure><h2 id="python队列、栈"><a href="#python队列、栈" class="headerlink" title="python队列、栈"></a>python队列、栈</h2><p>queue包下有以下四种队列形式</p><div class="table-container"><table><thead><tr><th><strong>队列方式</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr><td>queue.Queue</td><td>先进先出队列</td></tr><tr><td>queue.LifoQueue</td><td>后进先出队列</td></tr><tr><td>queue.PriorityQueue</td><td>优先级队列</td></tr><tr><td>queue.deque</td><td>双线队列</td></tr></tbody></table></div><p>另外collections.deque也实现了队列的性质，deque是双端队列（double-ended queue）的缩写，由于两端都能编辑，deque既可以用来实现栈（stack）也可以用来实现队列（queue）。</p><h2 id="set常用方法"><a href="#set常用方法" class="headerlink" title="set常用方法"></a>set常用方法</h2><p>判断一个元素是不是在集合中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s = <span class="built_in">set</span>([1,2,3,4])</span><br><span class="line">&gt;&gt;&gt; 1 <span class="keyword">in</span> s</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; 5 <span class="keyword">in</span> s</span><br><span class="line">False</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python基础语法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python基础语法 </tag>
            
            <tag> set </tag>
            
            <tag> queue </tag>
            
            <tag> stack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开始</title>
      <link href="/2022/02/08/begin/"/>
      <url>/2022/02/08/begin/</url>
      
        <content type="html"><![CDATA[<p>在大四的时候就想搭建一个自己的博客，一直拖着也没有弄。如今研一上学期已经结束，待在家中想着还是搭一个自己的博客吧，用来记录自己学习过程中的点滴和读研的日常。</p><p>刚开始想用springboot+vue搭一个自己的博客，在b站，github上面找了一些别人的博客看看，感觉蛮不错的，样式也挺美观的，但是如果自己弄一个前后端分离的博客，花费的时间会比较长，搜着搜着就发现一个Hexo的框架可以快速搭建一个无后台的博客。</p><p>于是花了几天的时间把Hexo的文档，和Butterfly主题的文档看了一点，搭了自己的一个博客。</p><p>搭完博客，总结这几天发现不需要写任何代码，如果需要添加一些自己的样式啥的，可能就要自己写一些CSS然后引入。现在的技术越来越智化了，都不要你写什么代码就能搭出一个系统来。不过Hexo博客也是有缺点的，没有自己的后台，就说明没有自己的数据库，管理就很不方便，如果资源（比如图片，视频）和博客内容非常多的话，那么就会变得卡顿起来。</p><p>我是利用GitHub pages弄了一个可以在线访问的地址，图片资源存放在本地，没有使用七牛云等服务器。目前打算是暂时利用Hexo作为自己的博客，等以后发现有比较好的spring boot+vue的博客可能就会重新开发一个。</p><p>好了，就算是自己的第一篇博客吧😂，希望以后能常更新，督促自己，一点一滴的成长😁。</p>]]></content>
      
      
      <categories>
          
          <category> 日常生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感想 </tag>
            
            <tag> 总结 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
